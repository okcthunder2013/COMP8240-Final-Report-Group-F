{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FastText Installation Guide for GitHub Codespaces\n",
    "\n",
    "This guide provides step-by-step instructions to install FastText in GitHub Codespaces, preparing the environment for text classification tasks.\n",
    "\n",
    "## Step 1: Update Packages and Install Dependencies\n",
    "\n",
    "First, update your packages and install the necessary dependencies for building FastText:\n",
    "\n",
    "```bash\n",
    "sudo apt-get update\n",
    "sudo apt-get install -y build-essential cmake\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Clone the FastText Repository\n",
    "```bash\n",
    "git clone https://github.com/facebookresearch/fastText.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Navigate to the FastText Directory\n",
    "```bash\n",
    "cd fastText\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting and Preparing the Dataset\n",
    "\n",
    "In this section, we’ll download the  dataset and prepare it for FastText. FastText requires a specific format for labeled data, so we’ll convert it accordingly.\n",
    "\n",
    "## Step 1: Download the ataset\n",
    "\n",
    "1. Go to the [PubMed 200k RCT dataset on Kaggle](https://www.kaggle.com/datasets/matthewjansen/pubmed-200k-rtc).\n",
    "2. Download the dataset file (`IMDB Dataset.csv`) and upload it to your working directory in GitHub Codespaces.\n",
    "\n",
    "## Step 2: Prepare the Data for FastText\n",
    "\n",
    "FastText requires each line of data to follow this format:\n",
    "\n",
    "```bash\n",
    "label<label> < text>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example:\n",
    "```bash\n",
    "label__results In these 80 surgical procedures , 147 SLNs were excised .\n",
    "__label__conclusions Results imply that study factors are more important than participant "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Convert Dataset to FastText Format\n",
    "\n",
    "Run the following Python script to convert the  dataset into the required format. This will create `train.txt` (80% of the data) for training and `test.txt` (20%) for validation.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "# Open the original dataset\n",
    "with open(\"C:\\\\Users\\\\asola\\\\OneDrive\\\\Desktop\\\\data science\\\\dataset.txt\", \"r\") as file:\n",
    "    lines = file.readlines()\n",
    "    \n",
    "# Initialize variables\n",
    "data = []\n",
    "current_abstract = []\n",
    "current_label = None\n",
    "\n",
    "# Process each line to extract labeled sections\n",
    "for line in lines:\n",
    "    if line.startswith(\"###\"):  # New abstract identifier\n",
    "        # Save the current abstract if there is one\n",
    "        if current_abstract:\n",
    "            data.append(' '.join(current_abstract))\n",
    "            current_abstract = []  # Reset for new abstract\n",
    "        current_label = None  # Reset label for a new abstract\n",
    "    elif line.strip():  # Non-empty line\n",
    "        # Match labels like BACKGROUND, METHODS, etc.\n",
    "        match = re.match(r'^(BACKGROUND|OBJECTIVE|METHODS|RESULTS|CONCLUSIONS)\\t', line)\n",
    "        if match:\n",
    "            # Set the label and start a new section\n",
    "            current_label = match.group(1).lower()\n",
    "            # Add the new line with the fastText label format\n",
    "            data.append(f\"__label__{current_label} {line.strip().split('\\t', 1)[-1]}\")\n",
    "        elif current_label:\n",
    "            # Continue appending lines within the same section\n",
    "            data[-1] += ' '+ line.strip()\n",
    "\n",
    "# Save the preprocessed data in fastText format\n",
    "with open('C:\\\\Users\\\\asola\\\\OneDrive\\\\Desktop\\\\data science\\\\New_Dataset.txt', 'w') as output_file:\n",
    "    for entry in data:\n",
    "        output_file.write(entry + '\\n')\n",
    "\n",
    "\n",
    "### split to test and train\n",
    "\n",
    "import random\n",
    "\n",
    "# Load the cleaned dataset\n",
    "with open(\"C:\\\\Users\\\\asola\\\\OneDrive\\\\Desktop\\\\data science\\\\New_Dataset.txt\", \"r\") as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# Shuffle the dataset to ensure randomness\n",
    "random.seed(42)  # Set a seed for reproducibility\n",
    "random.shuffle(lines)\n",
    "\n",
    "# Define the split ratio\n",
    "split_index = int(0.8 * len(lines))\n",
    "\n",
    "# Split the data\n",
    "train_lines = lines[:split_index]\n",
    "test_lines = lines[split_index:]\n",
    "\n",
    "# Save the train and test sets\n",
    "with open(\"C:\\\\Users\\\\asola\\\\OneDrive\\\\Desktop\\\\data science\\\\train.txt\", \"w\") as train_file:\n",
    "    train_file.writelines(train_lines)\n",
    "\n",
    "with open(\"C:\\\\Users\\\\asola\\\\OneDrive\\\\Desktop\\\\data science\\\\test.txt\", \"w\") as test_file:\n",
    "    test_file.writelines(test_lines)\n",
    "\n",
    "print(\"Dataset has been successfully split into 'train.txt' and 'test.txt'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Evaluating the Model\n",
    "\n",
    "Training the FastText model on the prepared  dataset and evaluate its performance on the test set.\n",
    "\n",
    "## Step 1: Train the Model\n",
    "\n",
    "Use the `train.txt` file to train a supervised FastText model. This command will save the trained model as `model_rct.bin`.\n",
    "\n",
    "```bash\n",
    "$ ./fasttext supervised -input train.txt -output model_rct\n",
    "Read 4M words\n",
    "Number of words:  104605\n",
    "Number of labels: 5\n",
    "Progress: 100.0% words/sec/thread:  808869 lr:  0.000000 avg.loss:  0.536392 ETA:   0h 0m 0s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "supervised: Tells FastText to run in supervised mode, suitable for classification tasks.\n",
    "\n",
    "-input train.txt: Specifies train.txt as the training data.\n",
    "\n",
    "-output model_imdb: Sets the output file name as model_rct.bin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Evaluate the Model\n",
    "\n",
    "After training, evaluate the model on test.txt to assess its accuracy:\n",
    "\n",
    "```bash \n",
    " $ ./fasttext test model_rct.bin test.txt\n",
    "\n",
    " N       36008\n",
    "P@1     0.817\n",
    "R@1     0.817"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The initial model without any preprocessing or tuning provides a relatively strong baseline performance for the RCT dataset. The balanced values for precision and recall indicate that the model is reasonably effective in identifying correct labels without significant bias towards false positives or false negatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test: Tells FastText to evaluate the model on test data.\n",
    "\n",
    "model_rct.bin: Specifies the trained model file.\n",
    "\n",
    "test.txt: Uses test.txt as the test data file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ```bash\n",
    "$ echo \"What procedures were used to coll\n",
    "ect the data in this study?\" |  ./fasttext predict model_rct.bin -\n",
    "__label__methods\n",
    "\n",
    "```bash\n",
    "$ echo \"What is the current understanding or previous research on this topic?\" |  ./fasttext predict model_rct.bin -\n",
    "__label__conclusions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above statement should have been background but the model is not trained properley so its giving wrong output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Step 4: Fine-tuning the model\n",
    "\n",
    "Changing the lr (Learning Rate) reduces avg.loss\n",
    "\n",
    "```bash\n",
    "./fasttext supervised -input train.txt -output model_rct -lr 0.5  \n",
    "Read 4M words\n",
    "Number of words:  104605\n",
    "Number of labels: 5\n",
    "Progress: 100.0% words/sec/thread:  772074 lr:  0.000000 avg.loss:  0.501036 ETA:   0h 0m 0s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ```bash\n",
    "./fasttext test model_rct.bin test.txt\n",
    "N       36008\n",
    "P@1     0.813\n",
    "R@1     0.813"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding data preprocessing (e.g., tokenization, lowercasing, removal of stop words) improves both precision and recall, raising both metrics to 0.817. This suggests that preprocessing helps the model better understand the structure and vocabulary of biomedical language in the RCT dataset, reducing misclassification and improving accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing the epochs also provides different output \n",
    "\n",
    "```bash\n",
    "./fasttext supervised -input train.txt -output model_rct -lr 0.5 -epoch 50 \n",
    "Read 4M words\n",
    "Number of words:  104605\n",
    "Number of labels: 5\n",
    "Progress: 100.0% words/sec/thread:  822120 lr:  0.000000 avg.loss:  0.270935 ETA:   0h 0m 0s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "```bash\n",
    "./fasttext test model_rct.bin test.txt\n",
    "N       36008\n",
    "P@1     0.764\n",
    "R@1     0.764"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Using Bigrams\n",
    "***'Bigram'*** the concatenation of 2 consecutive tokens or words. Similarly we often talk about n-gram to refer to the concatenation any n consecutive tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "./fasttext supervised -input train.txt -output model_rct -lr 1 -epoch 25 -wordNgrams 2 -bucket 200000 -dim 50 \n",
    "Read 4M words\n",
    "Number of words:  104605\n",
    "Number of labels: 5\n",
    "Progress: 100.0% words/sec/thread:  627638 lr:  0.000000 avg.loss:  0.298649 ETA:   0h 0m 0s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```bash\n",
    "N       36008\n",
    "P@1     0.816\n",
    "R@1     0.816"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding bigrams (two-word combinations) in addition to previous adjustments improves performance, raising both precision and recall to 0.816. The improvement suggests that bigrams help capture contextual relationships better, especially in medical language where certain phrases (e.g., \"adverse event,\" \"placebo effect\") have specific meanings that single-word analysis might miss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Using Hirachichal Softmax\n",
    "\n",
    "Hierarchical softmax is a computational technique used to speed up training, especially useful for large datasets. It can be enabled by adding the ***-loss*** hs option when training the model.\n",
    "```bash\n",
    "./fasttext supervised -input train.txt -output model_rct -lr 1 -epoch 25 -wordNgrams 2 -bucket 200000 -dim 50 -loss hs\n",
    "Read 4M words\n",
    "Number of words:  104605\n",
    "Number of labels: 5\n",
    "Progress: 100.0% words/sec/thread:  715916 lr:  0.000000 avg.loss:  0.051249 ETA:   0h 0m 0s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```bash\n",
    "\n",
    "N       36008\n",
    "P@1     0.808\n",
    "R@1     0.808"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using hierarchical softmax provides a strong balance between efficiency and accuracy on the RCT dataset, achieving both precision and recall at 0.808. This method allows the model to classify with high accuracy while reducing computational demands, making it suitable for applications that require rapid processing of biomedical text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7: Multi-label classification\n",
    "A convenient way to handle multiple labels is to use independent binary classifiers for each label. This can be done with ***-loss*** ***one-vs-all*** or ***-loss ova***.\n",
    "```bash\n",
    "./fasttext supervised -input train.txt -output model_imdb -lr 1 -epoch 25 -wordNgrams 2 -bucket 200000 -dim 50 -loss ova\n",
    "Read 4M words\n",
    "Number of words:  104605\n",
    "Number of labels: 5\n",
    "Progress: 100.0% words/sec/thread:  676900 lr:  0.000000 avg.loss:  0.117644 ETA:   0h 0m 0s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```bash\n",
    "N       36008\n",
    "P@1     0.808\n",
    "R@1     0.808"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moving to a multi-label classification setting yields a slight decrease in performance (precision and recall at 0.808) compared to Model 6. This could indicate that, although multi-label classification might be beneficial for datasets with overlapping or multiple labels per entry, it adds complexity without a substantial benefit for the RCT dataset, where each sentence may be adequately represented by a single label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now running the same sentence that the model gave wrong label :\n",
    "```bash\n",
    "echo \"What is the current understanding or previous research on this topic?\" |  ./fasttext predict model_rct.bin -\n",
    "__label__background\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially, this sentence was incorrectly labeled, highlighting limitations in the model’s initial configurations. Through iterative improvements, including data preprocessing, adjusting learning rate and epochs, and incorporating bigrams, the model's performance on the RCT dataset improved significantly. The final configuration, using hierarchical softmax, achieved an optimal balance between precision and computational efficiency. As a result, the model now correctly identifies __label__background for the sentence, demonstrating enhanced contextual understanding and accuracy on biomedical text after fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #For the RCT dataset, the single-label nature and use of top-1 prediction metrics (P@1 and R@1) cause precision and recall to be equal in all results. Each correctly predicted instance contributes identically to both precision and recall, making them converge to the same value. This is typical in single-label classification evaluated with top-1 metrics, especially in balanced datasets like the RCT, where every prediction either correctly or incorrectly matches a single true label.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
